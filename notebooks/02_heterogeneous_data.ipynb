{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3c48df",
   "metadata": {},
   "source": [
    "\n",
    "# Working with heterogeneous data\n",
    "\n",
    "In this notebook, we will present how to handle heterogeneous data. Usually, examples\n",
    "only show how to deal with numerical data but in practice, we often have to deal with\n",
    "a mix of numerical and categorical data.\n",
    "\n",
    "So let's look at the entire penguins dataset this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8a700",
   "metadata": {},
   "source": [
    "\n",
    "We see that we have some strings and numbers in this dataset. Let's set up a\n",
    "classification problem: we want to predict the species of the penguins given some\n",
    "numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75687be",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Region\",\n",
    "    \"Island\",\n",
    "    \"Culmen Length (mm)\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Body Mass (g)\",\n",
    "    \"Sex\",\n",
    "]\n",
    "target_name = \"Species\"\n",
    "X = df[feature_names]\n",
    "y = df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f74342a",
   "metadata": {},
   "source": [
    "\n",
    "Before to evaluate model through cross-validation, we will first look at model\n",
    "using a single split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dac2f3",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Fit a `LogisticRegression` model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88c9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46243815",
   "metadata": {},
   "source": [
    "\n",
    "So life is difficult. So let's start by looking at the numerical part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c50e77",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Fit a `LogisticRegression` model on the numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d2bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a1436c",
   "metadata": {},
   "source": [
    "\n",
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "logistic_regression = make_pipeline(SimpleImputer(), LogisticRegression())\n",
    "logistic_regression.fit(X_numeric, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be32d83",
   "metadata": {},
   "source": [
    "\n",
    "It works but we get a convergence warning. Let's check how many iterations were\n",
    "performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0616e",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we reached the maximum number of iterations. We can increase the number of\n",
    "iterations. Let's check which parameters we can set with the `get_params` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b53f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096127a0",
   "metadata": {},
   "source": [
    "\n",
    "We can set the `max_iter` parameter to a higher value through the variable\n",
    "`logisticregression__max_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802552",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.set_params(logisticregression__max_iter=10_000)\n",
    "logistic_regression.fit(X_numeric, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d311db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edc054",
   "metadata": {},
   "source": [
    "\n",
    "Now, the model converged but it required almost 2,500 iterations. The warning message\n",
    "mentioned that we could try to scale the data. Let's try to scale the data using a\n",
    "`StandardScaler`. We can then check if the convergence is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f88a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), SimpleImputer(), LogisticRegression()\n",
    ")\n",
    "logistic_regression.fit(X_numeric, y)\n",
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73852d8",
   "metadata": {},
   "source": [
    "\n",
    "It only requires 11 iterations. We can now evaluate the model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_numeric, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee0148",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's only consider the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = X.select_dtypes(exclude=\"number\")\n",
    "X_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff55f5d",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Think of a way to transform the string category into numerical data.\n",
    "Come with your own transform and evaluate the model using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec163f",
   "metadata": {},
   "source": [
    "\n",
    "We need to find a strategy to \"encode\" the categorical data into numerical data. The\n",
    "simplest strategy is to use an ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c028689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "X_encoded = ordinal_encoder.fit_transform(X_categorical)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d0a46",
   "metadata": {},
   "source": [
    "\n",
    "It replace a category by an integer. However, with linear models, it means that we\n",
    "would assume that the difference between two categories is the same. Also, there is\n",
    "an ordering imposed by this transformation.\n",
    "\n",
    "If this modelling assumption is not desired, we can use a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "X_encoded = one_hot_encoder.fit_transform(X_categorical)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9671f4",
   "metadata": {},
   "source": [
    "\n",
    "In this case, we create independent binary columns for each category. We therefore\n",
    "have an individual coefficient for each category. Usually, this is a more appropriate\n",
    "encoding for linear models.\n",
    "\n",
    "Let's use this encoding and evaluate the model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = make_pipeline(OneHotEncoder(), LogisticRegression())\n",
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_categorical, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2f2f8",
   "metadata": {},
   "source": [
    "\n",
    "We get an error for one of the split. This is due to the fact that some categories are\n",
    "not present in the test set. We can handle this issue by ignoring the unknown\n",
    "categories. This is given by a parameter in the `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.set_params(onehotencoder__handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_categorical, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b7f31",
   "metadata": {},
   "source": [
    "\n",
    "Now, we need to combine both numerical and categorical preprocessing and feed the\n",
    "output to a single linear model. The `ColumnTransformer` class is designed for this\n",
    "purpose: we provide a list of columns such that it will be transformed by a specific\n",
    "transformer (or a pipeline of transformers). This `ColumnTransformer` can be used as\n",
    "a preprocessing stage of a pipeline containing a linear model as the final stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ef72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns = selector(dtype_include=\"number\")\n",
    "numerical_columns(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "numerical_selector = selector(dtype_include=\"number\")\n",
    "categorical_selector = selector(dtype_exclude=\"number\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (make_pipeline(StandardScaler(), SimpleImputer()), numerical_selector),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_selector),\n",
    ")\n",
    "logistic_regression = make_pipeline(preprocessor, LogisticRegression())\n",
    "logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242b3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(logistic_regression, X, y, cv=10, return_train_score=True)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1516ba4",
   "metadata": {},
   "source": [
    "\n",
    "We gave basic preprocessing steps for linear model. However, there is another group\n",
    "of models that can handle heterogeneous data: tree-based models.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Looking at the documentation, create and evaluate a `HistGradientBoostingClassifier`\n",
    "model on the penguins dataset. You are free to create any preprocessing steps you\n",
    "want."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
