{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f965f70",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluate a predictive model\n",
    "\n",
    "In this first notebook, we show how to evaluate a predictive model.\n",
    "\n",
    "Let's consider a simple regression problem by loading a subset of the penguins\n",
    "dataset. Let's check what are the data available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8663f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/penguins_regression.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c1e60",
   "metadata": {},
   "source": [
    "\n",
    "In this dataset, we observe that we have two variables: the flipper length and the\n",
    "body mass of the penguins. The objective here is to create a predictive model allowing\n",
    "us to predict the body mass of a penguin based on its flipper length.\n",
    "\n",
    "First, we can have a look at the relationship between the flipper length and the body\n",
    "mass of the penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "_ = ax.set_title(\"Penguin Flipper Length vs Body Mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570c68f",
   "metadata": {},
   "source": [
    "\n",
    "Looking at this plot, we observe that there is a kind of linear relationship between\n",
    "the flipper length and the body mass of the penguins. We will start by fitting a\n",
    "linear regression model to this data.\n",
    "\n",
    "To do so, we will first prepare the data by creating the input data `X` and the target\n",
    "data `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415401f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Flipper Length (mm)\"]]\n",
    "y = df[\"Body Mass (g)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dimensions of X are: {X.shape}\")\n",
    "print(f\"The dimensions of y are: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9975cf2",
   "metadata": {},
   "source": [
    "\n",
    "Here, the matrix `X` only contains a single feature. However, in the future, we might\n",
    "want to add several features that allow use to predict the target `y`. The target `y`\n",
    "is a one-dimensional array here meaning that we only predict a single target. Note\n",
    "that in some cases, it is possible that the target to be predicted is a\n",
    "multi-dimensional array.\n",
    "\n",
    "Also, here we try to predict a continuous target. This is why we are in a regression\n",
    "setting. In other cases, we might want to predict a categorical target. This is called\n",
    "a classification problem.\n",
    "\n",
    "Let's start to fit a scikit-learn model that is a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c043ed",
   "metadata": {},
   "source": [
    "\n",
    "In scikit-learn, the method `fit` is used to train a model. In this case, it allows us\n",
    "to find the best parameters of the linear regression model to fit the data. These\n",
    "parameters are stored in the attributes `coef_` and `intercept_` of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_, linear_regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a3b5e",
   "metadata": {},
   "source": [
    "\n",
    "Let's now use this model to predict the body mass of the penguins based on their\n",
    "flipper length. We create a synthetic dataset of potential flipper length values and\n",
    "predict the body mass of the penguins using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e519b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_to_infer = pd.DataFrame({\"Flipper Length (mm)\": np.linspace(175, 230, 100)})\n",
    "y_pred = linear_regression.predict(X_to_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86626d40",
   "metadata": {},
   "source": [
    "\n",
    "The method `predict` allow us to get the prediction of the model on new data. Now,\n",
    "we plot the obtained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "ax.set_title(\"Penguin Flipper Length vs Body Mass\")\n",
    "ax.plot(X_to_infer, y_pred, linewidth=3, color=\"tab:orange\", label=\"Linear Regression\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511646db",
   "metadata": {},
   "source": [
    "\n",
    "This `LinearRegression` model is known to minimize the mean squared error. We can\n",
    "compute this metric to know what would be the error on the same dataset that we used\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"The mean squared error is: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e089e56",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Let's repeat the previous experiment by fitting again a linear model but this model\n",
    "is known as a quantile regression. You can import it from\n",
    "`sklearn.linear_model.QuantileRegressor`. Let's fit the median (look at the\n",
    "documentation to know which parameter to set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d0db7",
   "metadata": {},
   "source": [
    "\n",
    "Plot the prediction of the quantile regression model on the same plot as the linear\n",
    "regression model to have a quantitative comparison.\n",
    "Compute the mean squared error and compare it to the `LinearRegression` model.\n",
    "Compute the median absolute error and compare it to the `LinearRegression` model.\n",
    "Can you provide some insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a7882",
   "metadata": {},
   "source": [
    "\n",
    "Up to now, we have been evaluating the model on the same dataset but it does not tell\n",
    "us how well the model will generalize to new data. Let's imagine that we have a more\n",
    "complex model that make some data engineering. We can use a polynomial feature\n",
    "expansion to create a more complex model.\n",
    "\n",
    "Let's first demonstrate how to create a polynomial feature expansion with\n",
    "scikit-learn transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=5).set_output(transform=\"pandas\")\n",
    "standard_scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_scaled = standard_scaler.fit_transform(X)\n",
    "X_poly = polynomial_features.fit_transform(X_scaled)\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1936e2",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-learn transformers are models that have a `fit` and `transform` methods. The\n",
    "`fit` method will compute the required statistics to transform the data. The\n",
    "`transform` method will apply the transformation to the data.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fit a `LinearRegression` model on the `X_poly` data and predict the body mass of the\n",
    "penguins. Plot the prediction on the same plot as the linear regression model and the\n",
    "quantile regression model. Compute the mean squared error and compare it to the\n",
    "previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(X_poly, y)\n",
    "X_to_infer_poly = polynomial_features.transform(standard_scaler.transform(X_to_infer))\n",
    "y_pred_poly = linear_regression.predict(X_to_infer_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296dbec",
   "metadata": {},
   "source": [
    "\n",
    "Up to now, we have no way to compare the quality of this model with the previous\n",
    "model. To do so, we need to put ourself in a situation where we have a training set\n",
    "and a testing set. The training set is the set used to create the model while the\n",
    "testing set is used to evaluate the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce65a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "print(\n",
    "    \"Mean absolute error on the training set: \"\n",
    "    f\"{mean_absolute_error(y_train, linear_regression.predict(X_train)):.2f}\"\n",
    ")\n",
    "print(\n",
    "    \"Mean absolute error on the testing set: \"\n",
    "    f\"{mean_absolute_error(y_test, linear_regression.predict(X_test)):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c45e9",
   "metadata": {},
   "source": [
    "\n",
    "Now, by computing the mean absolute error on the testing set, we have an estimate of\n",
    "potential generalization power of our models. Eventually, we could keep the best model\n",
    "that leads to the lowest error on the testing set.\n",
    "\n",
    "However, we the results above, we have no idea of the variability of the error. We\n",
    "might have been lucky while creating the training and testing set. To have a better\n",
    "estimate of the error, we can use cross-validation: we will repeat the splitting of\n",
    "the data into training and testing set several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    linear_regression,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcd0e7",
   "metadata": {},
   "source": [
    "\n",
    "The `cross_validate` function allows us to make this cross-validation and store the\n",
    "important results in a Python dictionary. Note that scikit-learn uses a \"score\"\n",
    "convention: the higher the score, the better the model. Since we used error metrics,\n",
    "this convention will force us to use the negative values of the error.\n",
    "\n",
    "To access these metrics, we can pass a string to the `scoring` parameter. For error,\n",
    "we need to add the `neg_` prefix and it leads to negative values in the report.\n",
    "\n",
    "We negate the values to get back meaningful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a39210",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddaf463",
   "metadata": {},
   "source": [
    "\n",
    "So now, we have an estimate of the mean absolute error and its variability. We can\n",
    "compare it with the model using the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85230c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    linear_regression,\n",
    "    X_poly,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33fdc6",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the model using polynomial features has a lower mean absolute error\n",
    "than the linear model.\n",
    "\n",
    "However, we have an issue with the pattern used here. By scaling the full dataset and\n",
    "computing the polynomial features on the full dataset, we leak information from the\n",
    "the testing set to the training set. Therefore, the scores obtained might be too\n",
    "optimistic.\n",
    "\n",
    "We should therefore make the split before scaling the data and computing the\n",
    "polynomial features. Scikit-learn provides a `Pipeline` class that allows\n",
    "to chain several transformers and a final estimator.\n",
    "\n",
    "In this way, we can declare a pipeline that do not require any data during its\n",
    "declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_regression_poly = make_pipeline(\n",
    "    StandardScaler(), PolynomialFeatures(degree=5), LinearRegression()\n",
    ")\n",
    "linear_regression_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12a5bc",
   "metadata": {},
   "source": [
    "\n",
    "This sequence of transformers and final learner provide the same API as the final\n",
    "learner. Under the hood, it will call the proper methods when we call `fit` and\n",
    "`predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78400ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_poly.fit(X_train, y_train)\n",
    "linear_regression_poly[-1].coef_, linear_regression_poly[-1].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4456a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean absolute error on the training set: \"\n",
    "    f\"{mean_absolute_error(y_train, linear_regression_poly.predict(X_train)):.2f}\"\n",
    ")\n",
    "print(\n",
    "    \"Mean absolute error on the testing set: \"\n",
    "    f\"{mean_absolute_error(y_test, linear_regression_poly.predict(X_test)):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6a564",
   "metadata": {},
   "source": [
    "\n",
    "So now, we can safely use this model in the `cross_validate` function and pass the\n",
    "original data that will be transformed on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec1505",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    linear_regression_poly,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afd094",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Use a `sklearn.model_selection.RepeatedKFold` cross-validation strategy to evaluate\n",
    "the performance of the linear regression model and the polynomial regression model.\n",
    "\n",
    "The idea is to repeat several times to be able to plot a distribution of the test\n",
    "scores."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
