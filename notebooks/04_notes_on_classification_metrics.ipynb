{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1c2217",
   "metadata": {},
   "source": [
    "\n",
    "# Notes related to classification metrics\n",
    "\n",
    "This notebook goes a bit deeper on classification metrics. We are going to get back\n",
    "to the penguins dataset.\n",
    "\n",
    "Let's start by crafting a machine learning pipeline that we used in a previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44426e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Region\",\n",
    "    \"Island\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Body Mass (g)\",\n",
    "    \"Sex\",\n",
    "]\n",
    "target_name = \"Species\"\n",
    "X = df[feature_names]\n",
    "y = df[target_name]\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=\"object\").columns\n",
    "X[categorical_columns] = X[categorical_columns].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b19283",
   "metadata": {},
   "source": [
    "\n",
    "In addition, we will simplify the problem to a binary classification task by keeping\n",
    "only two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_keep = [\n",
    "    \"Adelie Penguin (Pygoscelis adeliae)\",\n",
    "    \"Chinstrap penguin (Pygoscelis antarctica)\",\n",
    "]\n",
    "X = X[y.isin(classes_to_keep)]\n",
    "y = y[y.isin(classes_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier(\n",
    "    categorical_features=\"from_dtype\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbadd1",
   "metadata": {},
   "source": [
    "\n",
    "We used in the previous notebook the `cross_validate` function to evaluate the\n",
    "performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af10b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(hist_gradient_boosting, X, y, cv=5, return_train_score=True)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec4daa",
   "metadata": {},
   "source": [
    "\n",
    "The score used by default in this case is the accuracy. However, accuracy is not\n",
    "always the best metric to evaluate the performance of a classifier. It is actually\n",
    "an issue with imbalanced datasets. We can check the ratio of the class on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e619c2f",
   "metadata": {},
   "source": [
    "\n",
    "So we already see if we predict always the majority class, we will get an accuracy of\n",
    "around 0.7. Therefore, when interpreting the accuracy obtained from the\n",
    "cross-validation, we should keep this in mind.\n",
    "\n",
    "Otherwise, we have some other metrics that we could use to evaluate the performance of\n",
    "a classifier. To check these metrics, we can check the `classification_report`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a52e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "hist_gradient_boosting.fit(X_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, hist_gradient_boosting.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b835f",
   "metadata": {},
   "source": [
    "\n",
    "These metrics are computed from the confusion matrix. To provide an example, we have\n",
    "the following confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8aeb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "display = ConfusionMatrixDisplay.from_predictions(\n",
    "    hist_gradient_boosting.predict(X_test), y_test\n",
    ")\n",
    "_ = display.ax_.set(\n",
    "    xticks=[0, 1],\n",
    "    yticks=[0, 1],\n",
    "    xticklabels=[\"Adelie\", \"Chinstrap\"],\n",
    "    yticklabels=[\"Adelie\", \"Chinstrap\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df06ba8",
   "metadata": {},
   "source": [
    "\n",
    "An important point to notice is that we used the method `predict` to compute these\n",
    "metrics. However, when dealing with classification, we could use the probabilistic\n",
    "properties of our classifier. Here, we will illustrate what are the probabilities\n",
    "output provided by the classifier and how those are transformed into a class.\n",
    "\n",
    "The method `predict_proba` provides the probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hist_gradient_boosting.predict_proba(X_test)\n",
    "pd.DataFrame(y_pred, columns=hist_gradient_boosting.classes_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76d142",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Try to find the relationship to go from the probabilities to the classes, or\n",
    "in other word to go from the output of `predict_proba` to the output of `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaed5b",
   "metadata": {},
   "source": [
    "\n",
    "So it means that the threshold of 0.5 is completely arbitrary. We could change this\n",
    "threshold and compute the confusion matrix and subsequent metrics. It will therefore\n",
    "create a curve.\n",
    "\n",
    "In classification, there is two main curves that are known: the ROC curve and the\n",
    "precision-recall curve. They usually show some trade-off regarding our classifier.\n",
    "Let's start to look at the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7587399",
   "metadata": {},
   "source": [
    "\n",
    "We can plot the value for the default threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(\n",
    "    y_test,\n",
    "    hist_gradient_boosting.predict(X_test),\n",
    "    pos_label=hist_gradient_boosting.classes_[1],\n",
    ")\n",
    "recall = recall_score(\n",
    "    y_test,\n",
    "    hist_gradient_boosting.predict(X_test),\n",
    "    pos_label=hist_gradient_boosting.classes_[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a96974",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "display.ax_.plot(recall, precision, marker=\"o\", label=\"Fixed threshold classifier\")\n",
    "display.ax_.legend()\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45c932",
   "metadata": {},
   "source": [
    "\n",
    "But we could try any other threshold and observe that we are moving on this curve.\n",
    "We use the `FixedThresholdClassifier` by setting the threshold to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import FixedThresholdClassifier\n",
    "\n",
    "classifier = FixedThresholdClassifier(hist_gradient_boosting, threshold=0.1)\n",
    "classifier.fit(X_train, y_train)\n",
    "precision = precision_score(\n",
    "    y_test, classifier.predict(X_test), pos_label=classifier.classes_[1]\n",
    ")\n",
    "recall = recall_score(\n",
    "    y_test, classifier.predict(X_test), pos_label=classifier.classes_[1]\n",
    ")\n",
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "display.ax_.plot(recall, precision, marker=\"o\", label=\"Fixed threshold classifier\")\n",
    "display.ax_.legend()\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f316ed",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "Make the same analysis by plotting the ROC curve. I let you check the documentation\n",
    "of scikit-learn to find the right classes to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c439c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2e6f4f6",
   "metadata": {},
   "source": [
    "\n",
    "The usage of these curves are chosen depending on the problem at end and the\n",
    "community.\n",
    "\n",
    "One important point concept to understand here is that these curves are evaluating\n",
    "the discriminative power of a classifier or in other words the ranking of the\n",
    "predictions.\n",
    "\n",
    "However, we don't evaluate if the probability estimates of the classifier are\n",
    "precise as known as well-calibrated. We can check the calibration of a classifier\n",
    "using the `CalibrationDisplay`. This is also known as a reliability diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibrationDisplay\n",
    "\n",
    "display = CalibrationDisplay.from_estimator(\n",
    "    hist_gradient_boosting, X, y, n_bins=5, pos_label=hist_gradient_boosting.classes_[1]\n",
    ")\n",
    "_ = display.ax_.set(xlabel=\"Mean predicted probability\", ylabel=\"Fraction of positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96f3af",
   "metadata": {},
   "source": [
    "\n",
    "Our classifier seems to be quite well calibrated. Let's create a model that is not\n",
    "well calibrated to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from skrub import tabular_learner\n",
    "\n",
    "logistic_regression = tabular_learner(LogisticRegression(C=1e-2)).fit(X_train, y_train)\n",
    "logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97408cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = CalibrationDisplay.from_estimator(\n",
    "    hist_gradient_boosting, X, y, n_bins=5, pos_label=hist_gradient_boosting.classes_[1]\n",
    ")\n",
    "CalibrationDisplay.from_estimator(\n",
    "    logistic_regression,\n",
    "    X,\n",
    "    y,\n",
    "    n_bins=5,\n",
    "    pos_label=logistic_regression.classes_[1],\n",
    "    ax=display.ax_,\n",
    "    name=\"LogisticRegression\",\n",
    ")\n",
    "_ = display.ax_.set(xlabel=\"Mean predicted probability\", ylabel=\"Fraction of positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b111df24",
   "metadata": {},
   "source": [
    "\n",
    "Here, the logistic regression does not follow the diagonal line. It means that the\n",
    "probabilities are not well calibrated.\n",
    "\n",
    "In practice, there are two metrics that could provide information regarding the\n",
    "quality of the probabilities: the log loss and the Brier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69478fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss_hgbdt = log_loss(y_test, hist_gradient_boosting.predict_proba(X_test))\n",
    "log_loss_rf = log_loss(y_test, logistic_regression.predict_proba(X_test))\n",
    "\n",
    "print(f\"Log loss of the HistGradientBoostingClassifier: {log_loss_hgbdt:.2f}\")\n",
    "print(f\"Log loss of the LogisticRegression: {log_loss_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "\n",
    "brier_score_hgbdt = brier_score_loss(\n",
    "    y_test,\n",
    "    hist_gradient_boosting.predict_proba(X_test)[:, 1],\n",
    "    pos_label=hist_gradient_boosting.classes_[1],\n",
    ")\n",
    "brier_score_rf = brier_score_loss(\n",
    "    y_test,\n",
    "    logistic_regression.predict_proba(X_test)[:, 1],\n",
    "    pos_label=logistic_regression.classes_[1],\n",
    ")\n",
    "\n",
    "print(f\"Brier score of the HistGradientBoostingClassifier: {brier_score_hgbdt:.2f}\")\n",
    "print(f\"Brier score of the LogisticRegression: {brier_score_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814f0696",
   "metadata": {},
   "source": [
    "\n",
    "We observed that the log loss and the Brier score are lower for the\n",
    "`HistGradientBoostingClassifier` than for the `LogisticRegression`.\n",
    "\n",
    "# Bonus section\n",
    "\n",
    "We will do this part if we have time. In the previous section, we saw that the\n",
    "`FixedThresholdClassifier` is a way to change the threshold of the classifier.\n",
    "However, we might want to find the best threshold that maximizes a metric.\n",
    "\n",
    "This is the job of the `TunedThresholdClassifierCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "display.ax_.legend()\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd29d18",
   "metadata": {},
   "source": [
    "\n",
    "Let's find on this curve, which model is maximizing the F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "\n",
    "# we need to pass pos_label because we have a binary classifier with classes other than\n",
    "# 0 and 1 or -1 and 1\n",
    "scorer = make_scorer(f1_score, pos_label=hist_gradient_boosting.classes_[1])\n",
    "tuned_threshold_classifier = TunedThresholdClassifierCV(\n",
    "    hist_gradient_boosting,\n",
    "    cv=3,\n",
    "    scoring=scorer,\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f35036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_threshold_classifier.best_threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(\n",
    "    y_test,\n",
    "    tuned_threshold_classifier.predict(X_test),\n",
    "    pos_label=tuned_threshold_classifier.classes_[1],\n",
    ")\n",
    "recall = recall_score(\n",
    "    y_test,\n",
    "    tuned_threshold_classifier.predict(X_test),\n",
    "    pos_label=tuned_threshold_classifier.classes_[1],\n",
    ")\n",
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "display.ax_.plot(recall, precision, marker=\"o\", label=\"Tuned threshold classifier\")\n",
    "display.ax_.legend()\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989a3da6",
   "metadata": {},
   "source": [
    "\n",
    "Here, we therefore found the threshold that maximizes the F1-score. We could do the\n",
    "same for other metrics.\n",
    "\n",
    "Actually, we can go a bit further by maximizing a metric while having a constraint on\n",
    "another metric. For instance, we might want to maximize the recall while having a\n",
    "precision greater than 0.5.\n",
    "\n",
    "In this case, we need to define a custom scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def max_recall_at_min_precision(y_true, y_pred, min_precision, pos_label):\n",
    "    precision = precision_score(y_true.tolist(), y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_true.tolist(), y_pred, pos_label=pos_label)\n",
    "    if precision < min_precision:\n",
    "        return -np.inf\n",
    "    return recall\n",
    "\n",
    "\n",
    "scorer = make_scorer(\n",
    "    max_recall_at_min_precision,\n",
    "    min_precision=0.5,\n",
    "    pos_label=hist_gradient_boosting.classes_[1],\n",
    ")\n",
    "tuned_threshold_classifier.set_params(scoring=scorer, store_cv_results=True).fit(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_threshold_classifier.best_threshold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af17ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(\n",
    "    y_test,\n",
    "    tuned_threshold_classifier.predict(X_test),\n",
    "    pos_label=tuned_threshold_classifier.classes_[1],\n",
    ")\n",
    "recall = recall_score(\n",
    "    y_test,\n",
    "    tuned_threshold_classifier.predict(X_test),\n",
    "    pos_label=tuned_threshold_classifier.classes_[1],\n",
    ")\n",
    "display = PrecisionRecallDisplay.from_estimator(hist_gradient_boosting, X_test, y_test)\n",
    "display.ax_.plot(recall, precision, marker=\"o\", label=\"Tuned threshold classifier\")\n",
    "display.ax_.legend()\n",
    "_ = display.ax_.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0175c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
