{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666cd263",
   "metadata": {},
   "source": [
    "\n",
    "# Working with heterogeneous data\n",
    "\n",
    "In this notebook, we will present how to handle heterogeneous data. Usually, examples\n",
    "only show how to deal with numerical data but in practice, we often have to deal with\n",
    "a mix of numerical and categorical data.\n",
    "\n",
    "So let's look at the entire penguins dataset this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bb003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32d12b",
   "metadata": {},
   "source": [
    "\n",
    "We see that we have some strings and numbers in this dataset. Let's set up a\n",
    "classification problem: we want to predict the species of the penguins given some\n",
    "numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053bfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Region\",\n",
    "    \"Island\",\n",
    "    \"Culmen Length (mm)\",\n",
    "    \"Culmen Depth (mm)\",\n",
    "    \"Flipper Length (mm)\",\n",
    "    \"Body Mass (g)\",\n",
    "    \"Sex\",\n",
    "]\n",
    "target_name = \"Species\"\n",
    "X = df[feature_names]\n",
    "y = df[target_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185dd6a",
   "metadata": {},
   "source": [
    "\n",
    "Before to evaluate model through cross-validation, we will first look at model\n",
    "using a single split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70db97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971b813",
   "metadata": {},
   "source": [
    "\n",
    "Let's consider a logistic regression model. Let's try to fit the model without\n",
    "preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e62834",
   "metadata": {},
   "source": [
    "\n",
    "We see that we have an error. Let's check if this is related to the type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef439f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c2ebb",
   "metadata": {},
   "source": [
    "\n",
    "So apparently yes... The `LogisticRegression` model from scikit-learn expects only\n",
    "numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f580a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809ceab",
   "metadata": {},
   "source": [
    "\n",
    "However, we observe that we have a mix of numerical and categorical data. Let's first\n",
    "concentrate on the numerical data and check if there are some subtle issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6659b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numeric = X.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.fit(X_numeric, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205dfee",
   "metadata": {},
   "source": [
    "\n",
    "Apparently, the data contains missing values and the linear model does not handle\n",
    "missing values natively. We can use a `SimpleImputer` to replace the missing values\n",
    "by the mean of the column.\n",
    "\n",
    "For this purpose, we can use a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "logistic_regression = make_pipeline(SimpleImputer(), LogisticRegression())\n",
    "logistic_regression.fit(X_numeric, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9e09e5",
   "metadata": {},
   "source": [
    "\n",
    "It works but we get a convergence warning. Let's check how many iterations were\n",
    "performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1232bcf",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we reached the maximum number of iterations. We can increase the number of\n",
    "iterations. Let's check which parameters we can set with the `get_params` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d373561",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d595b6bf",
   "metadata": {},
   "source": [
    "\n",
    "We can set the `max_iter` parameter to a higher value through the variable\n",
    "`logisticregression__max_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.set_params(logisticregression__max_iter=10_000)\n",
    "logistic_regression.fit(X_numeric, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd791a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b813b2",
   "metadata": {},
   "source": [
    "\n",
    "Now, the model converged but it required almost 2,500 iterations. The warning message\n",
    "mentioned that we could try to scale the data. Let's try to scale the data using a\n",
    "`StandardScaler`. We can then check if the convergence is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29027ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "logistic_regression = make_pipeline(\n",
    "    StandardScaler(), SimpleImputer(), LogisticRegression()\n",
    ")\n",
    "logistic_regression.fit(X_numeric, y)\n",
    "logistic_regression[-1].n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b11b54",
   "metadata": {},
   "source": [
    "\n",
    "It only requires 11 iterations. We can now evaluate the model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da558721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_numeric, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9c722",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's only consider the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_categorical = X.select_dtypes(exclude=\"number\")\n",
    "X_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff441448",
   "metadata": {},
   "source": [
    "\n",
    "We need to find a strategy to \"encode\" the categorical data into numerical data. The\n",
    "simplest strategy is to use an ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff79361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "X_encoded = ordinal_encoder.fit_transform(X_categorical)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f9e0b",
   "metadata": {},
   "source": [
    "\n",
    "It replace a category by an integer. However, with linear models, it means that we\n",
    "would assume that the difference between two categories is the same. Also, there is\n",
    "an ordering imposed by this transformation.\n",
    "\n",
    "If this modelling assumption is not desired, we can use a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "X_encoded = one_hot_encoder.fit_transform(X_categorical)\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd8cda",
   "metadata": {},
   "source": [
    "\n",
    "In this case, we create independent binary columns for each category. We therefore\n",
    "have an individual coefficient for each category. Usually, this is a more appropriate\n",
    "encoding for linear models.\n",
    "\n",
    "Let's use this encoding and evaluate the model using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = make_pipeline(OneHotEncoder(), LogisticRegression())\n",
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_categorical, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f404b",
   "metadata": {},
   "source": [
    "\n",
    "We get an error for one of the split. This is due to the fact that some categories are\n",
    "not present in the test set. We can handle this issue by ignoring the unknown\n",
    "categories. This is given by a parameter in the `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec176f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c670c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.set_params(onehotencoder__handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    logistic_regression, X_categorical, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[[\"train_score\", \"test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c258d6a",
   "metadata": {},
   "source": [
    "\n",
    "Now, we need to combine both numerical and categorical preprocessing and feed the\n",
    "output to a single linear model. The `ColumnTransformer` class is designed for this\n",
    "purpose: we provide a list of columns such that it will be transformed by a specific\n",
    "transformer (or a pipeline of transformers). This `ColumnTransformer` can be used as\n",
    "a preprocessing stage of a pipeline containing a linear model as the final stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a008f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "numerical_columns = selector(dtype_include=\"number\")\n",
    "numerical_columns(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87894102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "numerical_selector = selector(dtype_include=\"number\")\n",
    "categorical_selector = selector(dtype_exclude=\"number\")\n",
    "preprocessor = make_column_transformer(\n",
    "    (make_pipeline(StandardScaler(), SimpleImputer()), numerical_selector),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), categorical_selector),\n",
    ")\n",
    "logistic_regression = make_pipeline(preprocessor, LogisticRegression())\n",
    "logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(logistic_regression, X, y, cv=10, return_train_score=True)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6726f290",
   "metadata": {},
   "source": [
    "\n",
    "We gave basic preprocessing steps for linear model. However, there is another group\n",
    "of models that can handle heterogeneous data: tree-based models. Those models do not\n",
    "require scaling. Some in scikit-learn can even handle categorical data directly, if\n",
    "they are tagged as categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = categorical_selector(X)\n",
    "X[categorical_columns] = X[categorical_columns].astype(\"category\")\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ee2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier(\n",
    "    categorical_features=\"from_dtype\"\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    hist_gradient_boosting, X, y, cv=10, return_train_score=True\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166646bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
