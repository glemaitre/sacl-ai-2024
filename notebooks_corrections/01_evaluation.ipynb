{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66e8aa4",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluate a predictive model\n",
    "\n",
    "In this first notebook, we show how to evaluate a predictive model.\n",
    "\n",
    "Let's consider a simple regression problem by loading a subset of the penguins\n",
    "dataset. Let's check what are the data available in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/penguins_regression.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189fe37a",
   "metadata": {},
   "source": [
    "\n",
    "In this dataset, we observe that we have two variables: the flipper length and the\n",
    "body mass of the penguins. The objective here is to create a predictive model allowing\n",
    "us to predict the body mass of a penguin based on its flipper length.\n",
    "\n",
    "First, we can have a look at the relationship between the flipper length and the body\n",
    "mass of the penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "_ = ax.set_title(\"Penguin Flipper Length vs Body Mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51c4c3",
   "metadata": {},
   "source": [
    "\n",
    "Looking at this plot, we observe that there is a kind of linear relationship between\n",
    "the flipper length and the body mass of the penguins. We will start by fitting a\n",
    "linear regression model to this data.\n",
    "\n",
    "To do so, we will first prepare the data by creating the input data `X` and the target\n",
    "data `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Flipper Length (mm)\"]]\n",
    "y = df[\"Body Mass (g)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c774d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The dimensions of X are: {X.shape}\")\n",
    "print(f\"The dimensions of y are: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fdc27",
   "metadata": {},
   "source": [
    "\n",
    "Here, the matrix `X` only contains a single feature. However, in the future, we might\n",
    "want to add several features that allow use to predict the target `y`. The target `y`\n",
    "is a one-dimensional array here meaning that we only predict a single target. Note\n",
    "that in some cases, it is possible that the target to be predicted is a\n",
    "multi-dimensional array.\n",
    "\n",
    "Also, here we try to predict a continuous target. This is why we are in a regression\n",
    "setting. In other cases, we might want to predict a categorical target. This is called\n",
    "a classification problem.\n",
    "\n",
    "Let's start to fit a scikit-learn model that is a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318baa",
   "metadata": {},
   "source": [
    "\n",
    "In scikit-learn, the method `fit` is used to train a model. In this case, it allows us\n",
    "to find the best parameters of the linear regression model to fit the data. These\n",
    "parameters are stored in the attributes `coef_` and `intercept_` of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac46a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_, linear_regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7472e25",
   "metadata": {},
   "source": [
    "\n",
    "Let's now use this model to predict the body mass of the penguins based on their\n",
    "flipper length. We create a synthetic dataset of potential flipper length values and\n",
    "predict the body mass of the penguins using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_to_infer = pd.DataFrame({\"Flipper Length (mm)\": np.linspace(175, 230, 100)})\n",
    "y_pred = linear_regression.predict(X_to_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279ce6b",
   "metadata": {},
   "source": [
    "\n",
    "The method `predict` allow us to get the prediction of the model on new data. Now,\n",
    "we plot the obtained values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76560214",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "ax.set_title(\"Penguin Flipper Length vs Body Mass\")\n",
    "ax.plot(X_to_infer, y_pred, linewidth=3, color=\"tab:orange\", label=\"Linear Regression\")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f93197",
   "metadata": {},
   "source": [
    "\n",
    "This `LinearRegression` model is known to minimize the mean squared error. We can\n",
    "compute this metric to know what would be the error on the same dataset that we used\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "error = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"The mean squared error is: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998eab04",
   "metadata": {},
   "source": [
    "\n",
    "To emphasize the fact that `LinearRegression` minimizes the mean squared error, we can\n",
    "use another linear model that minimizes another metric. `QuantileRegressor` is a\n",
    "a linear model that minimizes the median absolute error. Indeed, this is an estimator\n",
    "of the median of the conditional distribution of the target.\n",
    "\n",
    "Let's fit such model and compare the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac480bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "quantile_regression = QuantileRegressor(alpha=0)  # alpha=0 to not regularize\n",
    "quantile_regression.fit(X, y)\n",
    "y_pred_quantile = quantile_regression.predict(X_to_infer)\n",
    "quantile_regression.coef_, quantile_regression.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84982a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "ax.set_title(\"Penguin Flipper Length vs Body Mass\")\n",
    "ax.plot(X_to_infer, y_pred, linewidth=3, color=\"tab:orange\", label=\"Linear Regression\")\n",
    "ax.plot(\n",
    "    X_to_infer,\n",
    "    y_pred_quantile,\n",
    "    linewidth=3,\n",
    "    color=\"tab:red\",\n",
    "    label=\"Quantile Regression\",\n",
    ")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63139b",
   "metadata": {},
   "source": [
    "\n",
    "We observe a slight difference between the two models found. We can compute the mean\n",
    "squared error of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(y, linear_regression.predict(X))\n",
    "print(f\"The mean squared error of LinearRegression is: {error:.2f}\")\n",
    "error = mean_squared_error(y, quantile_regression.predict(X))\n",
    "print(f\"The mean squared error of QuantileRegression is: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c034d",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the `QuantileRegressor` has a higher mean squared error than the\n",
    "`LinearRegression`. This is expected since the `QuantileRegressor` minimizes the\n",
    "median absolute error and not the mean squared error. We can compute the median\n",
    "absolute error to compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "error = median_absolute_error(y, linear_regression.predict(X))\n",
    "print(f\"The median absolute error of LinearRegression is: {error:.2f}\")\n",
    "error = median_absolute_error(y, quantile_regression.predict(X))\n",
    "print(f\"The median absolute error of QuantileRegression is: {error:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea56889",
   "metadata": {},
   "source": [
    "\n",
    "In terms of median absolute error, we observe that the `QuantileRegressor` has a lower\n",
    "error than the `LinearRegression`. It is in-line with the fact that the\n",
    "`QuantileRegressor` minimizes the median absolute error.\n",
    "\n",
    "Up to now, we have been evaluating the model on the same dataset but it does not tell\n",
    "us how well the model will generalize to new data. Let's imagine that we have a more\n",
    "complex model that make some data engineering. We can use a polynomial feature\n",
    "expansion to create a more complex model.\n",
    "\n",
    "Let's first demonstrate how to create a polynomial feature expansion with\n",
    "scikit-learn transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=5).set_output(transform=\"pandas\")\n",
    "standard_scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "X_scaled = standard_scaler.fit_transform(X)\n",
    "X_poly = polynomial_features.fit_transform(X_scaled)\n",
    "X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca225169",
   "metadata": {},
   "source": [
    "\n",
    "Scikit-learn transformers are models that have a `fit` and `transform` methods. The\n",
    "`fit` method will compute the required statistics to transform the data. The\n",
    "`transform` method will apply the transformation to the data.\n",
    "\n",
    "Now, let's use this transformed dataset to fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac993306",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.fit(X_poly, y)\n",
    "X_to_infer_poly = polynomial_features.transform(standard_scaler.transform(X_to_infer))\n",
    "y_pred_poly = linear_regression.predict(X_to_infer_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\n",
    "ax.set_title(\"Penguin Flipper Length vs Body Mass\")\n",
    "ax.plot(\n",
    "    X_to_infer, y_pred_poly, linewidth=3, color=\"tab:orange\", label=\"Linear Regression\"\n",
    ")\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc30d2a",
   "metadata": {},
   "source": [
    "\n",
    "We observed that while our model is linear, the fact that we created non-linear\n",
    "features lead to a non-linear relationship between the flipper length and the body\n",
    "mass of the penguins.\n",
    "\n",
    "However, we have no way to compare the quality of this model with the previous model.\n",
    "To do so, we need to put ourself in a situation where we have a training set and a\n",
    "testing set. The training set is the set used to create the model while the testing\n",
    "set is used to evaluate the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaee0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "linear_regression.fit(X_train, y_train)\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "print(\n",
    "    \"Mean absolute error on the training set: \"\n",
    "    f\"{mean_absolute_error(y_train, linear_regression.predict(X_train)):.2f}\"\n",
    ")\n",
    "print(\n",
    "    \"Mean absolute error on the testing set: \"\n",
    "    f\"{mean_absolute_error(y_test, linear_regression.predict(X_test)):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dd059",
   "metadata": {},
   "source": [
    "\n",
    "Now, by computing the mean absolute error on the testing set, we have an estimate of\n",
    "potential generalization power of our models. Eventually, we could keep the best model\n",
    "that leads to the lowest error on the testing set.\n",
    "\n",
    "However, we the results above, we have no idea of the variability of the error. We\n",
    "might have been lucky while creating the training and testing set. To have a better\n",
    "estimate of the error, we can use cross-validation: we will repeat the splitting of\n",
    "the data into training and testing set several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027cf67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    linear_regression,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023c7d5",
   "metadata": {},
   "source": [
    "\n",
    "The `cross_validate` function allows us to make this cross-validation and store the\n",
    "important results in a Python dictionary. Note that scikit-learn uses a \"score\"\n",
    "convention: the higher the score, the better the model. Since we used error metrics,\n",
    "this convention will force us to use the negative values of the error.\n",
    "\n",
    "To access these metrics, we can pass a string to the `scoring` parameter. For error,\n",
    "we need to add the `neg_` prefix and it leads to negative values in the report.\n",
    "\n",
    "We negate the values to get back meaningful values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed79a9",
   "metadata": {},
   "source": [
    "\n",
    "So now, we have an estimate of the mean absolute error and its variability. We can\n",
    "compare it with the model using the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    linear_regression,\n",
    "    X_poly,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb300167",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the model using polynomial features has a lower mean absolute error\n",
    "than the linear model.\n",
    "\n",
    "However, we have an issue with the pattern used here. By scaling the full dataset and\n",
    "computing the polynomial features on the full dataset, we leak information from the\n",
    "the testing set to the training set. Therefore, the scores obtained might be too\n",
    "optimistic.\n",
    "\n",
    "We should therefore make the split before scaling the data and computing the\n",
    "polynomial features. Scikit-learn provides a `Pipeline` class that allows\n",
    "to chain several transformers and a final estimator.\n",
    "\n",
    "In this way, we can declare a pipeline that do not require any data during its\n",
    "declaration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_regression_poly = make_pipeline(\n",
    "    StandardScaler(), PolynomialFeatures(degree=5), LinearRegression()\n",
    ")\n",
    "linear_regression_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238472d",
   "metadata": {},
   "source": [
    "\n",
    "This sequence of transformers and final learner provide the same API as the final\n",
    "learner. Under the hood, it will call the proper methods when we call `fit` and\n",
    "`predict` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f875b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_poly.fit(X_train, y_train)\n",
    "linear_regression_poly[-1].coef_, linear_regression_poly[-1].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Mean absolute error on the training set: \"\n",
    "    f\"{mean_absolute_error(y_train, linear_regression_poly.predict(X_train)):.2f}\"\n",
    ")\n",
    "print(\n",
    "    \"Mean absolute error on the testing set: \"\n",
    "    f\"{mean_absolute_error(y_test, linear_regression_poly.predict(X_test)):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85187e71",
   "metadata": {},
   "source": [
    "\n",
    "So now, we can safely use this model in the `cross_validate` function and pass the\n",
    "original data that will be transformed on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = cross_validate(\n",
    "    linear_regression_poly,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]\n",
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220d86b",
   "metadata": {},
   "source": [
    "\n",
    "In the previous cross-validation, we have only 5 estimations. We could repeat with\n",
    "more splits an more shuffling and display the distribution of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db86051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=100, random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    linear_regression,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[\"test_score\"] = -cv_results[\"test_score\"]\n",
    "cv_results[\"train_score\"] = -cv_results[\"train_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d83c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = cv_results[[\"train_score\", \"test_score\"]].plot.hist(bins=10, alpha=0.7)\n",
    "_ = ax.set_xlim(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[[\"train_score\", \"test_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad685bc",
   "metadata": {},
   "source": [
    "\n",
    "We can do the same with the polynomial features. We can also compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccd77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_regression_poly = make_pipeline(\n",
    "    PolynomialFeatures(degree=10), LinearRegression()\n",
    ")\n",
    "cv_results_poly = cross_validate(\n",
    "    linear_regression_poly,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    return_train_score=True,\n",
    ")\n",
    "cv_results_poly = pd.DataFrame(cv_results_poly)\n",
    "cv_results_poly[\"test_score\"] = -cv_results_poly[\"test_score\"]\n",
    "cv_results_poly[\"train_score\"] = -cv_results_poly[\"train_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0effa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = pd.concat(\n",
    "    [\n",
    "        cv_results[[\"test_score\"]].add_prefix(\"LinearRegression_\"),\n",
    "        cv_results_poly[[\"test_score\"]].add_prefix(\"PolynomialRegression_\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "ax = test_scores.plot.hist(bins=10, alpha=0.7)\n",
    "_ = ax.set_xlim(0, 500)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
